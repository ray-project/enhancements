
## Summary - Ray for federated learning and privacy-preserving computing
### General Motivation
Federated machine learning and privacy-preserving computing are getting wider. They're obviously distributed systems because they're always across parties(companies or ORGs.) To extend Ray ecosystem to federated learning and privacy-preserving computing domain, we should let users be easy to use Ray, for building their own federated learning and privacy-preserving computing applications without any concern on data privacy, task-attacks, illegal invasion, and etc.

PS: I also found another related issue: https://github.com/ray-project/ray/issues/25846

In this proposal, we'd like to build a connector layer on Ray, to provide the ability for users use Ray easily to build their such kind of system, with the secure issues addressed totally.

### Key requirements:
- Provide the ability for users to easily build their such kind of application on Ray.
- Setup different Ray clusters for different parties to avoid uncotrollable complex Ray protocols. 
  - We have tried to setup a single cross-party Ray cluster but failed for security reasons. It is difficult for parties to detect and prevent attacks, e.g. a malicious attacker (it could even be one of the parties) runs destructive code. The complex Ray protocols make it very difficult to enhance the security under single Ray cluster. 
- Have a unified and global-viewed programming mode for different parties.
- Data transmission across parties should be in push mode instead of pull mode. It's hard to prevent malicious attacker stealing data in pull mode. Push mode is much better since it's the responsibility of the party to decide whether to send data to others.
- Tasks should be driven in multi-controller mode. That means tasks should be driven by themselves(who are inside this party) instead of others.

### Should this change be within ray or outside?
No, there is no need to change ray repo, the steps should be:
1. Adding a new repo `RayFed` under `ray-project`: `ray-project/RayFed`
2. The package name is `rayfed`: pip install rayfed
3. importing lines should be `import fed`

## Stewardship
### Required Reviewers
@jovany-wang
### Shepherd of the Proposal (should be a senior committer)
@jovany-wang 

## Design and Architecture
The key words in this design are `multiple controller mode`, `restricted data perimeters`.

### User Examples
#### A Simple Example
```python
import fed

@fed.remote
class My:
    def incr(val):
        return self._val += val

@fed.remote(party="BOB")
def mean(val1, val2):
    return np.mean(val1, val2)


fed.init()

my1 = My.party("ALICE").remote()
my2 = My.party("BOB").remote()

val1 = my1.incr.remote(10)
val2 = my1.incr.remote(20)

result = mean.remote(val1, val2)
fed.get(result)

fed.shutdown()
```

From the very simple example code, we create an actor for ALICE and BOB, and then do an aggregation in BOB. Note that, the 2 actors are not in the same Ray cluster.

#### A Federated Learning Example - Diagram
![image](https://user-images.githubusercontent.com/19473861/206469265-e104888e-50c8-4313-aa18-f2caf0928ef1.png)

ALICE and BOB are doing their local training loop and synchronizing the weights every once in a while. In this proposal, it's easy to author the code on the top of Ray. ðŸ‘‡

#### A Federated Learning Example - Code
```python
import numpy as np
import tensorflow as tf
from sklearn.preprocessing import OneHotEncoder
from tensorflow import keras
import fed


@fed.remote
class My:
    def __init__(self):
        self._model = keras.Sequential()
        self._model.compile(optimizer=keras.Adam(), loss=crossentropy)

    def load_data(self, batch_size: int, epochs: int):
        self._dataset = _load_data_from_local()

    def train(self):
        x, y = next(self._dataset)
        with tf.GradientTape() as tape:
            y_pred = self.model(x, training=True)
            loss = self.model.compiled_loss(y, y_pred)
            self._model.optimizer.apply_gradients(zip(
                tape.gradient(loss, trainable_vars),
                self._model.trainable_variables))

    def get_weights(self, party):
        return self._model.get_weights()

    def update_weights(self, party, weights):
        self._model.set_weights(weights)
        return True


@fed.remote
def mean(party, x, y):
    return np.mean([x, y], axis=0)


def main():
    fed.init()
    epochs = 100, batch_size = 4

    my_alice = My.party("alice").remote()
    my_bob = My.party("bob").remote()

    my_alice.load_data.remote(batch_size, epochs)
    my_bob.load_data.remote(batch_size, epochs)

    num_batchs = int(150 / batch_size)
    for epoch in range(epochs):
        # Locally training inner party.
        for step in range(num_batchs):
            my_alice.train.remote()
            my_bob.train.remote()

        # Do weights aggregation and updating.
        w_a = my_alice.get_weights.remote()
        w_b = my_bob.get_weights.remote()
        w_mean = mean.party('alice').remote(w_a, w_b)
        result = fed.get(w_mean)
        print(f'Epoch {epoch} is finished, mean is {result}')
        n_wa = my_alice.update_weights.remote(w_mean)
        n_wb = my_bob.update_weights.remote(w_mean)

    fed.shutdown()
```

### Understanding the mechanism
In this section, we're introducing the deep dive of the above federated learning example, to help understand the mechanism of this connector layer.

Firstly,  ALICE and BOB companies need to create their Ray clusters, and expose one communication port to each other.  
And then, both ALICE and BOB need to run the example code in their Ray clusters(drive the DAG in multi-controller mode). Note that, the peer port and parties info should be passed via `fed.init()`.
![image](https://user-images.githubusercontent.com/19473861/206469580-468ea258-407e-40cf-b059-06a63a25c168.png)

Every Ray cluster will create the Ray tasks specified to its party, meanwhile, it will send the output data  to peer if the downstream Ray task are not specified to its own party. For example, in ALICE cluster, `mean.party('alice').remote(w_a, w_b)` will create a Ray task in ALICE cluster, but it wouldn't be created in BOB cluster. Due to `w_b` is an output data from BOB cluster, therefore, in ALICE cluster, it will be inserted a `recv_op` barrier to `mean` task, and in BOB cluster, it will be inserted a `send_op` barrier as a downstream of `w_b = my_bob.get_weights.remote()` task.

### Benefits 
Compared with running the DAG in one Ray cluster, we have the following significant benefits:
- It's very close to Ray native programming pattern. But the DAG could be ran across Ray clusters.
- Very restricted and clear data perimeters. we only transmit the data which is needed by others. And all of the data couldn't be fetched in any way if we don't allow.
- If we run the DAG in one Ray cluster, data transmission is in PULL-BASED mode. For example, if BOB invokes `ray.get(f.options("PARTY_ALICE").remote())`, the return object is pulled by BOB, as a result, ALICE don't have the knowledge for that. In this proposal, it's in PUSH-BASED mode. ALICE has the knowledge for that there is a data object will be sent to BOB. This is a significant advantage of multi-controller mode.
- All the disadvantages are addressed in this proposal: **code distribution**, **data privacy**, **task-attacks**, **illegal invasion**, **deserialization vulnerability**, and etc.
- Brings all Ray advantages to the local Ray cluster, like highly performance RPC, fault tolerance, task scheduling/resource management, and other Ray ecosystem librariesï¼ˆ Ray datasets, Ray train and Ray serveï¼‰ for local training.



## Compatibility, Deprecation, and Migration Plan
None.

## Test Plan and Acceptance Criteria
- Test should include unit tests and integration tests(release tests).
- Tests should be enabled in CI.
- Benchmark on across parties data transmission for typical federated learning case.
- Document page and necessary code comments.

## (Optional) Follow-on Work
- UX improvements for across parties exceptions passing.
- Provide single-controller mode for quickly building applications and debug inner one party.
- Performance optimization for other workloads.
- A unified gateway for ETH communication.
- Support TEE(Trust Execution Environment) device.